# Model Basics

## Introduction

No exercises.

## A simple model

### Exercises

```{r message = FALSE, warning = FALSE}
library(modelr)
```

*1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?*

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
```

The main message this question is trying to convey is that linear models are sensitive to outliers and influential data points. Plotting the simulated data and the linear model:

```{r}
coefs1a <- lm(y ~ x, data = sim1a) %>% coef()

ggplot(sim1a, aes(x, y)) +
  geom_point() +
  geom_abline(aes(intercept = coefs1a[1], slope = coefs1a[2]))
```

Lets replicate the datasets 500 times, and plot the fitted models (without the data points) together in the same plot:

```{r}
sims1_list <- replicate(500, 
                        {tibble(x = rep(1:10, each = 3),
                                y = x * 1.5 + 6 + rt(length(x), df = 2))},
                        simplify = FALSE)

coefs<- sims1_list %>%
  map(~lm(y ~ x, data = .)) %>%
  map(coef) %>%
  reduce(bind_rows) %>%
  rename(int = `(Intercept)`)

ggplot(sims1_list[[1]], aes(x, y)) +
  geom_blank() +
  geom_abline(aes(intercept = int, slope = x), data = coefs, alpha = .3)
```

We can see that there is a few fitted models that are different from the rest, and is the result of the random nature and the probabilty of generating outlying and deviant data points in the simulated datasets.

*2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:*

```{r}
measure_distance <- function(mod, data) {
  
  make_prediction <- function(mod, data){
    mod[1] + data$x * mod[2]
  }
  
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}
```

*Use optim() to fit this model to the simulated data above and compare it to the linear model.*

```{r}
optim(c(0,0), measure_distance, data = sim1a)$par
```

Compare with the coefficients from `lm()`:

```{r}
coefs1a
```

*3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optima. What’s the problem with optimising a three parameter model like this?*

```{r eval = FALSE}
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```

In this model, we only need either `a[1]` or `a[3]` because they both represent a constant term (y-intercept in this case). Including both will result in an infinite number of solutions.

## Visualising models

### Exercises

*1. Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()?*

```{r}
sim1_loess <- loess(y ~ x, sim1)

sim1_pred <- sim1 %>%
  data_grid(x) %>%
  add_predictions(sim1_loess)

ggplot(sim1, aes(x, y)) +
  geom_point() +
  geom_point(data = sim1_pred, aes(x, pred), color = "red", size = 2) +
  geom_smooth(se = FALSE, alpha = .5, method = "loess")
```

The curve produced by `geom_smooth()` with the argument `method` set to `"loess"` prefectly connects all the predictions made by the fitted loess model.

In other words, `geom_smooth()` silently fit a loess model to the data, make predictions, and plots the fitted curve all in one step.

*2. add_predictions() is paired with gather_predictions() and spread_predictions(). How do these three functions differ?*

`gather_predictions()` and `spread_predictions()` allow predictions to be made based on multiple modles, while `add_predictions()` only allows predictions to be made based on a single model. The only distinction between `gather_predictions()` and `spread_predictions()` is how the data frame is structured. `spread_predictions()` adds one column for each model; `gather_predictions()` adds two columns, one is model indicator and the other is the predicted values.

*3. What does `geom_ref_line()` do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important?*

`geom_ref_line()` comes from `modelr`, and is a wrapper to `geom_hline()` and `geom_vline()` from `ggplot2`. In this cause of residual plots, adding a reference line is useful to emphasize the residuals should be randomly scattering around the mean of 0.

*4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals?*

## Formulas and model families

### Exercises

*1. What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions?*

With intercept:

```{r}
mod2_int <- lm(y ~ x, data = sim2)
coef(mod2_int)
```

The coefficients for `xb`, `xc`, `xd` are interpreted as the expected difference from `xa`, and the intercept is interpreted as the expected value when x is equal to a.

Without intercept:

```{r}
mod2_noint <- lm(y ~ 0 + x, data = sim2)
coef(mod2_noint)
```

All the coefficients are interpreted as the expected values when x is equal to a, b, c, or d.

The predictions given by the two models are identifical (the function `near()` is used because it's a safer way of comparing floating point numbers):

```{r}
near(predict(mod2_int), predict(mod2_noint))
```

*2. Use model_matrix() to explore the equations generated for the models I fit to `sim3` and `sim4`. Why is \* a good shorthand for interaction?*

```{r}
model_matrix(data = sim4, y ~ x1 * x2)
```

* is a good shorthand for interaction because it automaically creates the terms for the main effects. For example, doing `y ~ x1 * x2` is equivalent to `y ~ x1 + x2 + x1*x2`. It saves lots of typing and looks more elegant. 

*3. Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.)*

```{r eval = FALSE}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
```

*4. For `sim4`, which of `mod1` and `mod2` is better? I think `mod2` does a slightly better job at removing patterns, but it's pretty subtle. Can you come up with a plot to support my claim?*

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)
```

```{r}
resids <- data.frame(mod1 = residuals(mod1),
                     mod2 = residuals(mod2)) %>%
  gather(key = "model", value = "resid")

ggplot(resids, aes(resid)) +
  geom_density(aes(color = model))
```

It still really hard to tell from the residual plots. Instead we can test if the difference in the residual sum of squares between the two models is significantly different:

```{r}
anova(mod1, mod2)
```

In this case, we have evidence to suggest `mod2` is better than `mod1`.

## Missing values

No exercises.

## Other model families

No exercises.